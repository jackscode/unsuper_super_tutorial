{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import read_csv\n",
    "from pandas.plotting import scatter_matrix\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import Image, display\n",
    "from collections import Counter\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\n",
    "full_path = 'iris.csv'\n",
    "dataframe = read_csv(full_path, names=names)\n",
    "\n",
    "# rows/cols of data\n",
    "print(\"rows and columns of imported data: {}\\n\".format(dataframe.shape))\n",
    "\n",
    "# first 20 entries\n",
    "print(\"first 20 entries of data:\\n{}\\n\".format(dataframe.head(20)))\n",
    "\n",
    "# descriptions\n",
    "print(\"some extra information on the data:\\n{}\\n\".format(dataframe.describe()))\n",
    "\n",
    "# class distribution\n",
    "print(\"class distribution:\\n{}\\n\".format(dataframe.groupby('class').size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image showing features of iris\n",
    "display(Image(filename='iris_features.png'))\n",
    "\n",
    "# histograms\n",
    "dataframe.hist()\n",
    "plt.show()\n",
    "\n",
    "# scatter plot matrix\n",
    "scatter_matrix(dataframe)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split-out validation dataset\n",
    "array = dataframe.values\n",
    "X = array[:,0:4]\n",
    "y = array[:,4]\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size=0.20, random_state=1, shuffle=True)\n",
    "\n",
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('Logistic Regression', LogisticRegression(solver='liblinear', multi_class='ovr')))\n",
    "models.append(('KNN', KNeighborsClassifier()))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('SVM', SVC(gamma='auto')))\n",
    "\n",
    "# evaluate each model in turn\n",
    "for name, model in models:\n",
    "    kfold = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    cross_val_results = cross_val_score(model, X_train, Y_train, cv=kfold, scoring='accuracy')\n",
    "    print('{:20} training results:{:.3f} {:.3f}'.format(name, cross_val_results.mean(), cross_val_results.std()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Using Single Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SVC(gamma='auto')\n",
    "model.fit(X_train, Y_train)\n",
    "test_predictions = model.predict(X_test)\n",
    "print(accuracy_score(Y_test, test_predictions))\n",
    "class_names = sorted(set(array[:,4]))\n",
    "# confusion matrix\n",
    "plot_confusion_matrix(model, X_test, Y_test,\n",
    "                                 display_labels=class_names,\n",
    "                                 cmap=plt.cm.Blues)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Trained Model to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_filename = \"sklearn_model.pkl\"\n",
    "with open(model_filename, 'wb') as file:\n",
    "    pickle.dump(model, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Trained Model from File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"sklearn_model.pkl\", 'rb') as file:\n",
    "    saved_model = pickle.load(file)\n",
    "\n",
    "test_predictions = saved_model.predict(X_test)\n",
    "print(accuracy_score(Y_test, test_predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the dataset location\n",
    "full_path = 'spambase.csv'\n",
    "# load the csv file as a pandas data frame\n",
    "dataframe = read_csv(full_path, header=None)\n",
    "# rows/cols of data\n",
    "print(\"rows and columns of imported data: {}\\n\".format(dataframe.shape))\n",
    "# first 20 entries\n",
    "print(\"first 5 entries of data:\\n{}\\n\".format(dataframe.head(5)))\n",
    "# class distribution\n",
    "target = dataframe.values[:,-1]\n",
    "counter = Counter(target)\n",
    "for k,v in counter.items():\n",
    "    per = v / len(target) * 100\n",
    "    print('Class=%d, Count=%d, Percentage=%.3f%%' % (k, v, per))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Functions to Keep Things Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "def load_dataset(full_path):\n",
    "    # load the dataset as a numpy array\n",
    "    data = read_csv(full_path, header=None)\n",
    "    # retrieve numpy array\n",
    "    data = data.values\n",
    "    # split into input and output elements\n",
    "    X, y = data[:, :-1], data[:, -1]\n",
    "    return X, y\n",
    "\n",
    "# define models to test\n",
    "def get_models():\n",
    "    models, names = list(), list()\n",
    "    # KNN\n",
    "    steps = [('s',StandardScaler()),('m',KNeighborsClassifier())]\n",
    "    models.append(Pipeline(steps=steps))\n",
    "    names.append('KNN')\n",
    "    # CART\n",
    "    models.append(DecisionTreeClassifier())\n",
    "    names.append('CART')\n",
    "    # RF\n",
    "    models.append(RandomForestClassifier(n_estimators=100))\n",
    "    names.append('Random Forest')\n",
    "    # Logistic Regression\n",
    "    models.append(LogisticRegression(solver='liblinear', multi_class='ovr'))\n",
    "    names.append('Logistic Regression')\n",
    "    \n",
    "    return models, names\n",
    "\n",
    "# evaluate a model\n",
    "def evaluate_model(X, y, model):\n",
    "    # define evaluation procedure\n",
    "    cv = StratifiedKFold(n_splits=10, random_state=1, shuffle=True)\n",
    "    # evaluate model\n",
    "    scores = cross_val_score(model, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "    return scores\n",
    "\n",
    "def timer(start,end):\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    return \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try Different Models on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the dataset\n",
    "X, y = load_dataset(full_path)\n",
    "# define models\n",
    "models, names = get_models()\n",
    "\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "    start = time.time()\n",
    "    # evaluate the model and store results\n",
    "    scores = evaluate_model(X, y, models[i])\n",
    "    # summarize performance\n",
    "    end = time.time()\n",
    "    print('{:20} mean:{:.3f} std dev:({:.3f}) time elapsed:{}'.format(names[i], mean(scores), std(scores), timer(start,end)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use PCA to Reduce Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimensions to 25\n",
    "X_reduced = PCA(n_components=25).fit_transform(X)\n",
    "\n",
    "# evaluate each model\n",
    "for i in range(len(models)):\n",
    "    start = time.time()\n",
    "    # evaluate the model and store results\n",
    "    scores = evaluate_model(X_reduced, y, models[i])\n",
    "    # summarize performance\n",
    "    end = time.time()\n",
    "    print('{:20} mean:{:.3f} std dev:({:.3f}) time elapsed:{}'.format(names[i], mean(scores), std(scores), timer(start,end)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Algorithms in Sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/stable/supervised_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UCI Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://archive.ics.uci.edu/ml/datasets.php"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
